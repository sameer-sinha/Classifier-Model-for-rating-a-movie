{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk,random\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading modules for text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the movie reviews corpus into documentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [(list(movie_reviews.words(fileids= files)),category) for category in movie_reviews.categories() for files in movie_reviews.fileids(categories= category)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(documents) == len(movie_reviews.fileids()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = [word for word in movie_reviews.words()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting keywords from the bag of words, to be used as features for the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize, remove stopwords, lemmatize, pos tagging\n",
    "def extract_keywords(word_list):\n",
    "    word= [w.lower() for w in word_list if w.isalpha() if w not in stop_words]\n",
    "    word = [lemmatiser.lemmatize(w) for w in word]\n",
    "    tagged_wordlist = nltk.pos_tag(word)\n",
    "    keyword = []\n",
    "    count = 0\n",
    "    \n",
    "# selecting adjectives and adverbs    \n",
    "    while count < len(tagged_wordlist):\n",
    "        if(tagged_wordlist[count][1] == \"JJ\" or tagged_wordlist[count][1] == \"RB\"):\n",
    "            keyword.append(tagged_wordlist[count][0])\n",
    "        count = count + 1\n",
    "    return keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_keywords = extract_keywords(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['teen', 'drive', 'accident', 'nightmare', 'critique', 'fuck', 'teen', 'touch', 'cool', 'bad', 'even', 'generally', 'highway', 'memento', 'good', 'bad', 'type', 'pretty', 'neat', 'terribly', 'well', 'main', 'simply', 'normal', 'fantasy', 'dream', 'back', 'dead', 'dead', 'strange', 'looooot', 'chase', 'weird', 'simply', 'personally', 'unravel', 'obviously', 'big', 'secret', 'want', 'completely', 'final', 'even', 'meantime', 'really', 'sad', 'actually', 'half', 'strangeness', 'start', 'little', 'still', 'guess', 'bottom', 'always', 'sure', 'even', 'secret', 'mean', 'melissa', 'away', 'lazy', 'okay', 'really', 'need', 'u', 'different', 'insight', 'apparently', 'away', 'decent', 'teen', 'fuck', 'somewhere', 'guess', 'little', 'pretty', 'good', 'exact', 'character', 'american', 'new', 'entire', 'actually', 'overall', 'rarely', 'pretty', 'redundant', 'pretty', 'cool', 'oh', 'apparently', 'still', 'hot', 'also', 'ever', 'skip', 'nightmare', 'elm', 'highway']\n"
     ]
    }
   ],
   "source": [
    "print(feature_keywords[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_keywords_freq_dist = nltk.FreqDist(feature_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('even', 2568), ('good', 2388), ('much', 2045), ('also', 1967), ('well', 1895), ('first', 1828), ('really', 1558), ('little', 1492), ('bad', 1395), ('never', 1374), ('new', 1292), ('many', 1268), ('great', 1150), ('u', 1072), ('big', 1064), ('still', 1053), ('however', 989), ('back', 935), ('real', 915), ('enough', 902), ('old', 887), ('last', 852), ('actually', 837), ('long', 835), ('almost', 820), ('ever', 776), ('funny', 750), ('young', 743), ('right', 735), ('original', 705), ('quite', 649), ('far', 635), ('high', 631), ('rather', 621), ('american', 608), ('yet', 605), ('always', 586), ('special', 572), ('hard', 569), ('instead', 565), ('black', 542), ('probably', 539), ('human', 538), ('away', 531), ('together', 521), ('pretty', 510), ('sure', 491), ('whole', 482), ('perhaps', 464), ('second', 457), ('especially', 456), ('completely', 440), ('different', 430), ('small', 429), ('simply', 428), ('several', 419), ('give', 411), ('true', 410), ('entire', 408), ('dead', 408), ('soon', 402), ('main', 400), ('comic', 395), ('else', 387), ('final', 383), ('unfortunately', 382), ('wrong', 381), ('next', 377), ('full', 375), ('often', 370), ('alien', 362), ('certainly', 361), ('finally', 350), ('interesting', 347), ('maybe', 344), ('able', 339), ('later', 336), ('top', 336), ('nice', 331), ('open', 329), ('white', 322), ('classic', 321), ('short', 314), ('screen', 312), ('evil', 309), ('nearly', 307), ('early', 303), ('major', 302), ('exactly', 297), ('close', 292), ('obvious', 291), ('already', 289), ('deep', 278), ('beautiful', 272), ('live', 271), ('perfect', 270), ('sometimes', 270), ('strong', 268), ('quickly', 255), ('truly', 255)]\n"
     ]
    }
   ],
   "source": [
    "print(feature_keywords_freq_dist.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_features =[x for (x,y) in feature_keywords_freq_dist.most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['even', 'good', 'much', 'also', 'well', 'first', 'really', 'little', 'bad', 'never', 'new', 'many', 'great', 'u', 'big', 'still', 'however', 'back', 'real', 'enough', 'old', 'last', 'actually', 'long', 'almost', 'ever', 'funny', 'young', 'right', 'original', 'quite', 'far', 'high', 'rather', 'american', 'yet', 'always', 'special', 'hard', 'instead', 'black', 'probably', 'human', 'away', 'together', 'pretty', 'sure', 'whole', 'perhaps', 'second', 'especially', 'completely', 'different', 'small', 'simply', 'several', 'give', 'true', 'entire', 'dead', 'soon', 'main', 'comic', 'else', 'final', 'unfortunately', 'wrong', 'next', 'full', 'often', 'alien', 'certainly', 'finally', 'interesting', 'maybe', 'able', 'later', 'top', 'nice', 'open', 'white', 'classic', 'short', 'screen', 'evil', 'nearly', 'early', 'major', 'exactly', 'close', 'obvious', 'already', 'deep', 'beautiful', 'live', 'perfect', 'sometimes', 'strong', 'quickly', 'truly']\n"
     ]
    }
   ],
   "source": [
    "print(word_features[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding features for a given document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_features(document) :\n",
    "    # document here is going to be first part of tuple i.e just a list of words\n",
    "    words = set(extract_keywords(document))\n",
    "    # Converting list to set, inludes all the words and not the amount of words\n",
    "    features = {}\n",
    "    # empty dictionary\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)  # returns true or false based on the words presence in top 3000\n",
    "        # w, from word_features, is the key of features dictionary\n",
    "        # w in words, from words i.e set(document) returns a boolean true or false\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating labelled featureset for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(featuresets) == len(movie_reviews.fileids()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('retracing', False), ('rant', False)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(featuresets[0][0].items(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresets[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cutoff(split = 0.75):\n",
    "    return int(len(featuresets) * split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set = featuresets[:cutoff()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing_set = featuresets[cutoff():]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Naive Bayes Algo accuracy percent: 77.8\n",
      "Most Informative Features\n",
      "                   inept = True              neg : pos    =     14.6 : 1.0\n",
      "               marvelous = True              pos : neg    =     11.5 : 1.0\n",
      "                flawless = True              pos : neg    =     10.2 : 1.0\n",
      "                  annual = True              pos : neg    =      9.5 : 1.0\n",
      "               ludicrous = True              neg : pos    =      9.4 : 1.0\n",
      "            inexplicably = True              neg : pos    =      9.1 : 1.0\n",
      "                   oscar = True              pos : neg    =      8.9 : 1.0\n",
      "                 idiotic = True              neg : pos    =      8.6 : 1.0\n",
      "              derivative = True              neg : pos    =      8.5 : 1.0\n",
      "             outstanding = True              pos : neg    =      8.3 : 1.0\n",
      "                      na = True              pos : neg    =      8.2 : 1.0\n",
      "                   elite = True              pos : neg    =      8.2 : 1.0\n",
      "                thematic = True              pos : neg    =      8.2 : 1.0\n",
      "                    rent = True              neg : pos    =      7.8 : 1.0\n",
      "             magnificent = True              pos : neg    =      7.5 : 1.0\n",
      "             wonderfully = True              pos : neg    =      7.3 : 1.0\n",
      "                  feeble = True              neg : pos    =      7.1 : 1.0\n",
      "               illogical = True              neg : pos    =      7.1 : 1.0\n",
      "                    echo = True              pos : neg    =      6.9 : 1.0\n",
      "                     neo = True              pos : neg    =      6.9 : 1.0\n",
      "               exquisite = True              pos : neg    =      6.9 : 1.0\n",
      "                  mature = True              pos : neg    =      6.9 : 1.0\n",
      "                guardian = True              pos : neg    =      6.9 : 1.0\n",
      "              ridiculous = True              neg : pos    =      6.5 : 1.0\n",
      "                 sincere = True              pos : neg    =      6.5 : 1.0\n",
      "MNB_classifier accuracy percent: 81.0\n",
      "BernoulliNB_classifier accuracy percent: 77.2\n",
      "LogisticRegression_classifier accuracy percent: 78.2\n",
      "SGDClassifier_classifier accuracy percent: 74.6\n",
      "SVC_classifier accuracy percent: 48.8\n",
      "LinearSVC_classifier accuracy percent: 75.0\n",
      "NuSVC_classifier accuracy percent: 79.0\n"
     ]
    }
   ],
   "source": [
    "# training various models\n",
    "\n",
    "NB_classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "print(\"Original Naive Bayes Algo accuracy percent:\", (nltk.classify.accuracy(NB_classifier, testing_set))*100)\n",
    "NB_classifier.show_most_informative_features(25)\n",
    "\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"MNB_classifier accuracy percent:\", (nltk.classify.accuracy(MNB_classifier, testing_set))*100)\n",
    "\n",
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(training_set)\n",
    "print(\"BernoulliNB_classifier accuracy percent:\", (nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100)\n",
    "\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)\n",
    "\n",
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(training_set)\n",
    "print(\"SGDClassifier_classifier accuracy percent:\", (nltk.classify.accuracy(SGDClassifier_classifier, testing_set))*100)\n",
    "\n",
    "SVC_classifier = SklearnClassifier(SVC())\n",
    "SVC_classifier.train(training_set)\n",
    "print(\"SVC_classifier accuracy percent:\", (nltk.classify.accuracy(SVC_classifier, testing_set))*100)\n",
    "\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(\"LinearSVC_classifier accuracy percent:\", (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)\n",
    "\n",
    "NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "NuSVC_classifier.train(training_set)\n",
    "print(\"NuSVC_classifier accuracy percent:\", (nltk.classify.accuracy(NuSVC_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Creating a VoteClassifier, which is basically a voting system. Have used odd number of classifiers.\n",
    "+ Each algorithm gets one vote, and the classification that has the most votes is the chosen one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "\n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "\n",
    "# creating a confidence method, which can tally the votes for and against the winning vote    \n",
    "#  eg. 3/5 votes for positive is weaker than 5/5 votes\n",
    "\n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voted_classifier accuracy percent: 79.2\n"
     ]
    }
   ],
   "source": [
    "voted_classifier = VoteClassifier(NB_classifier,\n",
    "                                  NuSVC_classifier,\n",
    "                                  LinearSVC_classifier,\n",
    "                                  SGDClassifier_classifier,\n",
    "                                  MNB_classifier,\n",
    "                                  BernoulliNB_classifier,\n",
    "                                  LogisticRegression_classifier)\n",
    "\n",
    "print(\"voted_classifier accuracy percent:\", (nltk.classify.accuracy(voted_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples for the confidence percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('kenneth', False), ('however', False), ('tarkovskian', False), ('generously', False), ('sabbatical', False), ('plotline', False), ('countryish', False), ('jamileh', False), ('armstrong', False), ('applegate', False), ('actually', False), ('english', False), ('australia', False), ('serum', False), ('vocally', False)]\n"
     ]
    }
   ],
   "source": [
    "print(random.sample(testing_set[0][0].items(), 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pacific', False), ('grungy', False), ('convent', False), ('undermining', False), ('scamper', False), ('actual', False), ('exemplified', False), ('wholly', False), ('hideous', False), ('form', False), ('disappointingly', False), ('tourist', False), ('continue', False), ('provocatively', False), ('genetic', False)]\n"
     ]
    }
   ],
   "source": [
    "print(random.sample(testing_set[1][0].items(), 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification: pos Confidence %: 100.0\n",
      "Classification: neg Confidence %: 100.0\n",
      "Classification: neg Confidence %: 85.71428571428571\n",
      "Classification: pos Confidence %: 57.14285714285714\n",
      "Classification: neg Confidence %: 100.0\n",
      "Classification: pos Confidence %: 85.71428571428571\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification:\", voted_classifier.classify(testing_set[0][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[0][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[1][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[1][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[2][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[2][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[3][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[3][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[4][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[4][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[5][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[5][0])*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_review(review_input):\n",
    "    review_word_list = [words for words in word_tokenize(review_input)]\n",
    "    review_features = find_features(review_word_list)\n",
    "\n",
    "    if(voted_classifier.classify(features=review_features) == 'pos'):\n",
    "        sentiment = \"Positive Sentiment\"\n",
    "    else:\n",
    "        sentiment = \"Negative Sentiment\"\n",
    "    return(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the review paragraphs here : \n",
      "\n",
      "The last few Scorsese pics left me a little disappointed. I had begun to think Marty had become a 'gun for hire' and that his brilliance may have been spent (his earlier works were some of the best movies ever made). I attended a screening of The Wolf of Wall Street this evening, and was expecting to be unimpressed. I am happy to say I was completely blown away. This pic is Marty at his best. I laughed, I cringed, I related (with fond memories as well as a bit of guilt) and I TOTALLY believed every unbelievable moment. A good book, a great screenplay and a delightful cast were formed and molded into what I believe should get Scorsese a best director Oscar, and likely a Best Picture Award for the movie. Leo DiCaprio has grown into a versatile actor and his creation of this super hero dirtbag's roller coaster ride in this crazy (true) story is really honest and delightfully entertaining. Jonah Hill pulled out all the stops too and this is definitely his best work. Thank you Mr. Scorsese for delivering the goods so brilliantly!\n"
     ]
    }
   ],
   "source": [
    "input1 = input(\"Please enter the review paragraphs here : \\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive Sentiment'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_review(input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the review paragraphs here : \n",
      "\n",
      "It's hard to find the words to explain how TRULY AWFUL this film is. I'll try to do a list:  1) There's no context: They never show the victims of the fraud. We see the sales effort but not the people they're selling to. How can you do a movie about people perpetrating a fraud without showing the fraud & its effects???   2) There's no character development: They all start out as disgusting creeps and they all end up being disgusting creeps.   3) There's not much of a plot: It's 2 and 1/2 hours of debauchery and then 1/2 hour of getting caught. The debauchery part goes on forever and gets boring really fast. Not to mention disturbing & disgusting. Did Scorsese really make this movie just to show all this debauchery? What's the point of showing 2 and 1/2 hours of it? We get the point that they are gross lunatics pretty fast. Why keep going with seemingly endless variations of it? There is no point to it.   So, when all is said & done, this is basically a movie about debauchery. It should have been called \"Satyrs of Long Island\" instead of \"Wolves of Wall Street\" because these turkeys operated from LI and there's practically nothing in the movie about actual Wall Street firms.\n"
     ]
    }
   ],
   "source": [
    "input2 = input(\"Please enter the review paragraphs here : \\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative Sentiment'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_review(input2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py33]",
   "language": "python",
   "name": "conda-env-py33-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
