{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, inspect, os, random, pickle\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords, movie_reviews\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents = [(list(movie_reviews.words(fileids= files)),category) for category in movie_reviews.categories() for files in movie_reviews.fileids(categories= category)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = [word for word in movie_reviews.words()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_keywords(word_list):\n",
    "    word= [w.lower() for w in word_list if w.isalpha() if w not in stop_words]\n",
    "    word = [lemmatiser.lemmatize(w) for w in word]\n",
    "    tagged_wordlist = nltk.pos_tag(word)\n",
    "    keyword = []\n",
    "    count = 0\n",
    "    \n",
    "    while count < len(tagged_wordlist):\n",
    "        if(tagged_wordlist[count][1] == \"JJ\" or tagged_wordlist[count][1] == \"RB\"):\n",
    "            keyword.append(tagged_wordlist[count][0])\n",
    "        count = count + 1\n",
    "    return keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_keywords = extract_keywords(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['teen', 'drive', 'accident', 'nightmare', 'critique', 'fuck', 'teen', 'touch', 'cool', 'bad', 'even', 'generally', 'highway', 'memento', 'good', 'bad', 'type', 'pretty', 'neat', 'terribly', 'well', 'main', 'simply', 'normal', 'fantasy', 'dream', 'back', 'dead', 'dead', 'strange', 'looooot', 'chase', 'weird', 'simply', 'personally', 'unravel', 'obviously', 'big', 'secret', 'want', 'completely', 'final', 'even', 'meantime', 'really', 'sad', 'actually', 'half', 'strangeness', 'start', 'little', 'still', 'guess', 'bottom', 'always', 'sure', 'even', 'secret', 'mean', 'melissa', 'away', 'lazy', 'okay', 'really', 'need', 'u', 'different', 'insight', 'apparently', 'away', 'decent', 'teen', 'fuck', 'somewhere', 'guess', 'little', 'pretty', 'good', 'exact', 'character', 'american', 'new', 'entire', 'actually', 'overall', 'rarely', 'pretty', 'redundant', 'pretty', 'cool', 'oh', 'apparently', 'still', 'hot', 'also', 'ever', 'skip', 'nightmare', 'elm', 'highway']\n"
     ]
    }
   ],
   "source": [
    "print(feature_keywords[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_keywords_freq_dist = nltk.FreqDist(feature_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('even', 2568), ('good', 2388), ('much', 2045), ('also', 1967), ('well', 1895), ('first', 1828), ('really', 1558), ('little', 1492), ('bad', 1395), ('never', 1374), ('new', 1292), ('many', 1268), ('great', 1150), ('u', 1072), ('big', 1064), ('still', 1053), ('however', 989), ('back', 935), ('real', 915), ('enough', 902), ('old', 887), ('last', 852), ('actually', 837), ('long', 835), ('almost', 820), ('ever', 776), ('funny', 750), ('young', 743), ('right', 735), ('original', 705), ('quite', 649), ('far', 635), ('high', 631), ('rather', 621), ('american', 608), ('yet', 605), ('always', 586), ('special', 572), ('hard', 569), ('instead', 565), ('black', 542), ('probably', 539), ('human', 538), ('away', 531), ('together', 521), ('pretty', 510), ('sure', 491), ('whole', 482), ('perhaps', 464), ('second', 457), ('especially', 456), ('completely', 440), ('different', 430), ('small', 429), ('simply', 428), ('several', 419), ('give', 411), ('true', 410), ('entire', 408), ('dead', 408), ('soon', 402), ('main', 400), ('comic', 395), ('else', 387), ('final', 383), ('unfortunately', 382), ('wrong', 381), ('next', 377), ('full', 375), ('often', 370), ('alien', 362), ('certainly', 361), ('finally', 350), ('interesting', 347), ('maybe', 344), ('able', 339), ('top', 336), ('later', 336), ('nice', 331), ('open', 329), ('white', 322), ('classic', 321), ('short', 314), ('screen', 312), ('evil', 309), ('nearly', 307), ('early', 303), ('major', 302), ('exactly', 297), ('close', 292), ('obvious', 291), ('already', 289), ('deep', 278), ('beautiful', 272), ('live', 271), ('perfect', 270), ('sometimes', 270), ('strong', 268), ('truly', 255), ('quickly', 255)]\n"
     ]
    }
   ],
   "source": [
    "print(feature_keywords_freq_dist.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_features =[x for (x,y) in feature_keywords_freq_dist.most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['even', 'good', 'much', 'also', 'well', 'first', 'really', 'little', 'bad', 'never', 'new', 'many', 'great', 'u', 'big', 'still', 'however', 'back', 'real', 'enough', 'old', 'last', 'actually', 'long', 'almost', 'ever', 'funny', 'young', 'right', 'original', 'quite', 'far', 'high', 'rather', 'american', 'yet', 'always', 'special', 'hard', 'instead', 'black', 'probably', 'human', 'away', 'together', 'pretty', 'sure', 'whole', 'perhaps', 'second', 'especially', 'completely', 'different', 'small', 'simply', 'several', 'give', 'true', 'entire', 'dead', 'soon', 'main', 'comic', 'else', 'final', 'unfortunately', 'wrong', 'next', 'full', 'often', 'alien', 'certainly', 'finally', 'interesting', 'maybe', 'able', 'top', 'later', 'nice', 'open', 'white', 'classic', 'short', 'screen', 'evil', 'nearly', 'early', 'major', 'exactly', 'close', 'obvious', 'already', 'deep', 'beautiful', 'live', 'perfect', 'sometimes', 'strong', 'truly', 'quickly']\n"
     ]
    }
   ],
   "source": [
    "print(word_features[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16468"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(feature_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16468"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_features(document) :\n",
    "    # document here is going to be first part of tuple i.e just a list of words\n",
    "    words = set(extract_keywords(document))\n",
    "    # Converting list to set, inludes all the words and not the amount of words\n",
    "    features = {}\n",
    "    # empty dictionary\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)  # returns true or false based on the words presence in top 3000\n",
    "        # w, from word_features, is the key of features dictionary\n",
    "        # w in words, from words i.e set(document) returns a boolean true or false\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cutoff(split = 0.75):\n",
    "    return int(len(featuresets) * split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_set = featuresets[:cutoff()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing_set = featuresets[cutoff():]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myClassifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent: 80.4\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(myClassifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "               ludicrous = True              neg : pos    =     16.5 : 1.0\n",
      "                   blame = True              neg : pos    =     11.8 : 1.0\n",
      "             outstanding = True              pos : neg    =     11.3 : 1.0\n",
      "                    pure = True              pos : neg    =     10.9 : 1.0\n",
      "                  sloppy = True              neg : pos    =     10.3 : 1.0\n",
      "              unoriginal = True              neg : pos    =      9.8 : 1.0\n",
      "             wonderfully = True              pos : neg    =      9.6 : 1.0\n",
      "                   inept = True              neg : pos    =      9.5 : 1.0\n",
      "                 insipid = True              neg : pos    =      9.1 : 1.0\n",
      "              underneath = True              pos : neg    =      8.9 : 1.0\n",
      "             beautifully = True              pos : neg    =      8.9 : 1.0\n",
      "            refreshingly = True              pos : neg    =      8.2 : 1.0\n",
      "              derivative = True              neg : pos    =      7.7 : 1.0\n",
      "                    rent = True              neg : pos    =      7.7 : 1.0\n",
      "                   inane = True              neg : pos    =      7.7 : 1.0\n",
      "                  mature = True              pos : neg    =      7.6 : 1.0\n",
      "               marvelous = True              pos : neg    =      7.3 : 1.0\n",
      "              whatsoever = True              neg : pos    =      7.1 : 1.0\n",
      "                 enjoyed = True              pos : neg    =      6.9 : 1.0\n",
      "                thematic = True              pos : neg    =      6.9 : 1.0\n",
      "                     neo = True              pos : neg    =      6.9 : 1.0\n",
      "                   fairy = True              pos : neg    =      6.9 : 1.0\n",
      "           unintentional = True              neg : pos    =      6.7 : 1.0\n",
      "                  verbal = True              pos : neg    =      6.5 : 1.0\n",
      "                   eager = True              pos : neg    =      6.5 : 1.0\n",
      "                     sat = True              neg : pos    =      6.4 : 1.0\n",
      "                 villain = True              neg : pos    =      6.4 : 1.0\n",
      "                passable = True              neg : pos    =      6.4 : 1.0\n",
      "           unimaginative = True              neg : pos    =      6.4 : 1.0\n",
      "                  crappy = True              neg : pos    =      6.4 : 1.0\n",
      "          excruciatingly = True              neg : pos    =      6.4 : 1.0\n",
      "               illogical = True              neg : pos    =      6.4 : 1.0\n",
      "                 mcgowan = True              neg : pos    =      6.4 : 1.0\n",
      "                     eve = True              neg : pos    =      6.4 : 1.0\n",
      "                  expect = True              pos : neg    =      6.3 : 1.0\n",
      "                    keen = True              pos : neg    =      6.3 : 1.0\n",
      "                   panic = True              pos : neg    =      6.3 : 1.0\n",
      "                 delight = True              pos : neg    =      6.3 : 1.0\n",
      "                  denial = True              pos : neg    =      6.3 : 1.0\n",
      "                   fargo = True              pos : neg    =      6.3 : 1.0\n",
      "               testament = True              pos : neg    =      6.3 : 1.0\n",
      "                guardian = True              pos : neg    =      6.3 : 1.0\n",
      "            effortlessly = True              pos : neg    =      6.1 : 1.0\n",
      "                   ideal = True              pos : neg    =      6.1 : 1.0\n",
      "                poignant = True              pos : neg    =      6.1 : 1.0\n",
      "             brilliantly = True              pos : neg    =      5.8 : 1.0\n",
      "                    lean = True              pos : neg    =      5.7 : 1.0\n",
      "                   vocal = True              pos : neg    =      5.7 : 1.0\n",
      "                     pen = True              neg : pos    =      5.7 : 1.0\n",
      "                marginal = True              neg : pos    =      5.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "myClassifier.show_most_informative_features(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_review(review_input):\n",
    "    review_word_list = [words for words in word_tokenize(review_input)]\n",
    "    review_features = find_features(review_word_list)\n",
    "\n",
    "    if(myClassifier.classify(review_features) == 'pos'):\n",
    "        sentiment = \"Positive Sentiment\"\n",
    "    else:\n",
    "        sentiment = \"Negative Sentiment\"\n",
    "    return(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the review paragraphs here : \n",
      "\n",
      "The last few Scorsese pics left me a little disappointed. I had begun to think Marty had become a 'gun for hire' and that his brilliance may have been spent (his earlier works were some of the best movies ever made). I attended a screening of The Wolf of Wall Street this evening, and was expecting to be unimpressed. I am happy to say I was completely blown away. This pic is Marty at his best. I laughed, I cringed, I related (with fond memories as well as a bit of guilt) and I TOTALLY believed every unbelievable moment. A good book, a great screenplay and a delightful cast were formed and molded into what I believe should get Scorsese a best director Oscar, and likely a Best Picture Award for the movie. Leo DiCaprio has grown into a versatile actor and his creation of this super hero dirtbag's roller coaster ride in this crazy (true) story is really honest and delightfully entertaining. Jonah Hill pulled out all the stops too and this is definitely his best work. Thank you Mr. Scorsese for delivering the goods so brilliantly!\n"
     ]
    }
   ],
   "source": [
    "input1 = input(\"Please enter the review paragraphs here : \\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive Sentiment'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_review(input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the review paragraphs here : \n",
      "\n",
      "It's hard to find the words to explain how TRULY AWFUL this film is. I'll try to do a list:  1) There's no context: They never show the victims of the fraud. We see the sales effort but not the people they're selling to. How can you do a movie about people perpetrating a fraud without showing the fraud & its effects???   2) There's no character development: They all start out as disgusting creeps and they all end up being disgusting creeps.   3) There's not much of a plot: It's 2 and 1/2 hours of debauchery and then 1/2 hour of getting caught. The debauchery part goes on forever and gets boring really fast. Not to mention disturbing & disgusting. Did Scorsese really make this movie just to show all this debauchery? What's the point of showing 2 and 1/2 hours of it? We get the point that they are gross lunatics pretty fast. Why keep going with seemingly endless variations of it? There is no point to it.   So, when all is said & done, this is basically a movie about debauchery. It should have been called \"Satyrs of Long Island\" instead of \"Wolves of Wall Street\" because these turkeys operated from LI and there's practically nothing in the movie about actual Wall Street firms.\n"
     ]
    }
   ],
   "source": [
    "input2 = input(\"Please enter the review paragraphs here : \\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative Sentiment'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_review(input2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Input Reviews](http://www.imdb.com/title/tt0993846/reviews?ref_=tt_urv)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py33]",
   "language": "python",
   "name": "conda-env-py33-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
